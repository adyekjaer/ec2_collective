#!/usr/bin/env python
# coding: utf-8

# S3 stuff
from boto.s3.key import Key
from boto.s3.connection import S3Connection
from boto.exception import S3ResponseError

# SQS stuff
from boto.sqs.connection import SQSConnection
from boto.sqs.regioninfo import SQSRegionInfo
from boto.sqs import connect_to_region

# Forking stuff
from multiprocessing import Process, Queue, active_children
from Queue import Empty

from socket import gethostname
import simplejson as json
import subprocess
from threading import Timer
import time
import sched
import signal
import sys
import yaml
import os
import stat
import getopt
import logging

CFILE='/etc/ec2_collective/ec2-cagent.json'

def terminate_process(signum, frame):
    logging.info ('Process asked to exit...')
    sys.exit(1)

signal.signal(signal.SIGTERM, terminate_process)

def usage():
    print >>sys.stderr, '    Usage:'
    print >>sys.stderr, '    ' + sys.argv[0] + '\n\n -f, --foreground\trun script in foreground\n -h, --help\tthis help\n -l, --logfile\tlogfile path ( /var/log/ec2-cagent.log )\n -p, --pidfile\tpath to pidfile (/var/run/ec2-cagent.pid)'
    sys.exit(1)

def set_logging():

    logformat = '%(asctime)s [%(levelname)s] %(message)s'
    logging.basicConfig(level=logging.INFO, format=logformat)
    logging.getLogger('boto').setLevel(logging.CRITICAL)

    if CFG['general']['log_level'] not in ['INFO', 'WARN', 'ERROR', 'DEBUG', 'CRITICAL' ]:
        print >>sys.stderr, 'Log level: ' + CFG['general']['log_level'] + ' is invalid'

    if CFG['general']['log_level'] == 'INFO':
        logging.getLogger().setLevel(logging.INFO)
    elif CFG['general']['log_level'] == 'WARN':
        logging.getLogger().setLevel(logging.WARNING)
    elif CFG['general']['log_level'] == 'ERROR':
        logging.getLogger().setLevel(logging.ERROR)
    elif CFG['general']['log_level'] == 'DEBUG':
        logging.getLogger().setLevel(logging.DEBUG)
    elif CFG['general']['log_level'] == 'CRITICAL':
        logging.getLogger().setLevel(logging.CRITICAL)

def initialize():

    get_config()
    set_logging()

    foreground=False
    logfile='/var/log/ec2-cagent.log'
    pidfile='/var/run/ec2-cagent.pid'

    try:
        opts, args = getopt.getopt(sys.argv[1:], 'hfl:p:', ['foreground', 'help', 'logfile', 'pidfile'])

    except getopt.GetoptError, err:
        print >>sys.stderr, str(err) 
        return 1

    for o, a in opts:
        if o in ('-h', '--help'):
            usage()
        elif o in ('-f', '--foreground'):
            foreground=True
        elif o in ('-l', '--logfile'):
            logfile=a
        elif o in ('-p', '--pidfile'):
            pidfile=a

    return (foreground, logfile, pidfile)

def get_yaml_facts (yaml_queue, action=None):
    logging.debug('Looking for new facts')

    yaml_file = CFG['general']['yaml_facts_path']

    if not os.path.exists(yaml_file):
        logging.error( yaml_file + ' file does not exist')
        return False
    else:
        try:
            stat = os.stat(yaml_file)
            fileage = int(stat.st_mtime)
        except OSError, e:
            logging.error('Could not stat ' + str(yaml_file) + ' ' + str(e))
            return False

    dataMap = {}

    if fileage >= (time.time() - (CFG['general']['yaml_facts_refresh'] * 1.5 )) or action == 'initial':
        logging.debug('Will try to reload facts')

        f = open(yaml_file, 'r')
        dataMap = yaml.safe_load(f)
        f.close()
    
        if len(dataMap) > 0:
            logging.info('Facts reloaded')
            logging.debug(dataMap)
            yaml_queue.put(dataMap)
            return True
        else:
            logging.debug('Facts are empty')
	    return False
    else:
        return False

def cli_func (message):

	try:
           logging.debug ('Executing: ' + str(message['cmd']))
           o = subprocess.Popen(message['cmd'], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
           output = o.communicate()[0]
           rc = o.poll()

        except OSError, e:
            output = ('Failed to execute ' + message['cmd'] + ' (%d) %s \n' % (e.errno, e.strerror))
            rc = e.errno 

        response={'func': message['func'], 'output': output, 'rc': rc, 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
        return response

def write_to_file (payload, identifier):
    script_file = '/tmp/' + 'ec2-cagent-' + identifier

    try:
        f = open (script_file, 'w')
        f.write (payload)
        f.close()
    except IOError, e:
        logging.error ('Failed to write payload to ' + script_file + ' (%d) %s \n' % (e.errno, e.strerror))
        return (False, 'Failed to write payload to file ' + script_file)

    os.chmod(script_file, stat.S_IRWXU)

    return (True, script_file)

def fetch_s3_file ( s3_path, identifier ):

    bucket_name=os.path.dirname(s3_path)
    file_name=os.path.basename(s3_path)

    logging.debug ('Fetching script ' + file_name + ' from s3 bucket ' + bucket_name )

    try:
        s3conn =S3Connection()
    except S3ResponseError:
        logging.error ('Failed to connect to S3')
        return (False, 'Failed to connect to S3')
    
    try:
        bucket = s3conn.get_bucket(bucket_name)
    except S3ResponseError:
        logging.error ('Failed to get bucket ' + bucket_name )
        return (False, 'Failed to get bucket ' + bucket_name )

    k = Key(bucket)
    k.key = file_name

    try:
        payload = k.get_contents_as_string()
    except S3ResponseError, e:
        logging.error ('Failed to get file from bucket ' + str(e))
        return (False, 'Failed to get file from bucket')

    return write_to_file(payload, identifier )

def script_func (message):

    # Put script on filesystem
    if message['func'] == 's3':
        (file_written, script_file ) = fetch_s3_file(message['cmd'], message['batch_msg_id'])
    else:
        (file_written, script_file ) = write_to_file(message['payload'], message['batch_msg_id'])

    if file_written is False:
        # script_file will include error message
        response={'func': message['func'], 'output': script_file, 'rc': '255', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
        return response
  
    command=script_file 

    # If we are carrying script parameters, please use them
    if message['script_param'] is not None:
        command = str(command) + ' ' + str(message['script_param'])

    try:
        logging.debug ('Executing script: ' + str(message['cmd']) + ' with params: ' + str(message['script_param']))
        o = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        output = o.communicate()[0]
        rc = o.poll()

    except OSError, e:
        logging.error ('Failed to execute ' + message['cmd'] )
        output = ('Failed to execute ' + message['cmd'] + ' (%d) %s \n' % (e.errno, e.strerror))
        rc = e.errno 

    try:
        logging.debug ('Deleting file with script '  + script_file)
        os.remove(script_file)
    except OSError, e:
        logging.error ('Failed to delete script file ' + script_file + ' (%d) %s \n' % (e.errno, e.strerror))

    response={'func': message['func'], 'output': output, 'rc': rc, 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
    return response

def receive_sqs_msg ( sqs_read_queue, task_queue, poll_control_queue, yaml_facts, scheduled_queue ):
    global READ_MSGS
    global READ_MSGS_LTS

    logging.debug('Looking for new messages on SQS')

    msgs = sqs_read_queue.get_messages(num_messages=10, visibility_timeout=0)

    for msg in msgs:
        READ_MSGS_LTS=time.time()

	if msg.id in READ_MSGS:
            continue
        else:
            READ_MSGS.append(msg.id)
   
        message=json.loads(msg.get_body())
        batch_msg_id = message['batch_msg_id']
        wf = message['wf']
        wof = message['wof']
        message['orgid'] = msg.id

        # We send multiple duplicate messages - lets avoid handling all of them
	if batch_msg_id in READ_MSGS:
            continue
        else:
            READ_MSGS.append(batch_msg_id)
  
	if fact_lookup(wf, wof, yaml_facts ):
            logging.debug('SQS messages did not pass filter')
            READ_MSGS.append(msg.id)
            continue

        logging.debug('New valid SQS message received')

        if int(message['schedule']) > time.time() and message['func'] not in [ 'discovery', 'ping', 'count' ]:
            logging.debug('Putting SQS message on scheduled queue and task queue')
            scheduled_queue.put(message)
            message['is_scheduled']=True
            task_queue.put(message)
        else:
            logging.debug('Putting SQS message on task queue')
            task_queue.put(message)

        poll_control_queue.put(CFG['general']['sqs_att_poll_interval'])


def process_msg ( message ):

    cmd_str = str(message['cmd'])
    func = str(message['func'])
    ts = str(message['ts'])

    # If this is a scheduled task
    if 'is_scheduled' in message and message['is_scheduled'] is True:
        logging.info('Responding to scheduled task')
        response={'func': func, 'output': 'Task scheduled...', 'rc': '0', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
        return response

    if func in [ 'discovery', 'ping', 'count' ]:
        logging.info("Performing ping reply")
        response={'func': func, 'output': ts, 'rc': '0', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
    elif func == 'cli':
        logging.info("Performing command execution")
        response = cli_func(message)
    elif func in [ 'script', 's3' ]:
        logging.info('Performing ' + func + ' execution')
        response = script_func (message) 
    else:
        logging.error('Unknown functionality: ' + func)
        response =  'Unknown command ' + cmd_str
        response={'func': func, 'output': response, 'rc': '0', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};

    return response

def receive_queue_msg ( task_queue, done_queue ):

    for message in iter(task_queue.get, 'STOP'):
        logging.debug('Task read from task queue')
        response = process_msg(message)
        done_queue.put(response)

    logging.debug('Worker received STOP signal - terminating')

def write_sqs_msg (response, sqs_write_queue):
 
    response=json.dumps(response)
    message = sqs_write_queue.new_message(response)
    
    # Write message 5 times to make sure receiver gets it
    written=False
    for i in range(0, 3):
        org = sqs_write_queue.write(message)
        if org.id is None and written is False:
            logging.error ('Failed to write response message to SQS')
        else:
            logging.debug('Wrote response to SQS')
            written=True

    return written

def get_config():
    # CFILE 
    if not os.path.exists(CFILE):
        logging.error ( CFILE + ' file does not exist')
        sys.exit(1)
    
    try:
        f = open(CFILE, 'r')
    except IOError, e:
        logging.error ('Failed to execute ' + message['cmd'] + ' (%d) %s \n' % (e.errno, e.strerror))
    
    try:
        global CFG
        CFG=json.load(f)
    except (TypeError, ValueError), e:
        logging.error ('Error in configuration file')
        sys.exit(1)

def daemonize (foreground, pidfile, stdin='/dev/null', stdout='/dev/null', stderr='/dev/null' ):

    if foreground is True:
        logging.info ('Running in the foreground')
        return

    try:
        pid = os.fork( )
        if pid > 0:
            sys.exit(0) # Exit first parent.
    except OSError, e:
        sys.stderr.write("fork #1 failed: (%d) %s\n" % (e.errno, e.strerror))
        sys.exit(1)
    # Decouple from parent environment.
    os.chdir("/")
    os.umask(0)
    os.setsid( )
    # Perform second fork.
    try:
        pid = os.fork( )
        if pid > 0:
            sys.exit(0) # Exit second parent.
    except OSError, e:
        sys.stderr.write("fork #2 failed: (%d) %s\n" % (e.errno, e.strerror))
        sys.exit(1)
    # The process is now daemonized, redirect standard file descriptors.
    for f in sys.stdout, sys.stderr: f.flush( )
    si = file(stdin, 'r')
    so = file(stdout, 'a+')
    se = file(stderr, 'a+', 0)
    os.dup2(si.fileno( ), sys.stdin.fileno( ))
    os.dup2(so.fileno( ), sys.stdout.fileno( ))
    os.dup2(se.fileno( ), sys.stderr.fileno( ))

    try:
        f = open (pidfile, 'w')
        f.write (str(os.getpid()))
    except IOError, e:
        sys.stderr.write("Failed to write pid to pidfile (%s): (%d) %s\n" % (pidfile, e.errno, e.strerror))
        sys.exit(1)

    sys.stdout.write('Daemon started with pid %d\n' % os.getpid( ) )

    sys.stderr.flush()
    sys.stdout.flush()

def fact_lookup (wf, wof, yaml_facts ):

    # True - Skip
    # False - Process

    # WOF
    # Return True if we have the fact ( just on should skip message )
    # Return False if we don't have the fact

    # WF
    # Return False if we have all the fact ( all facts must match )
    # Return True if we don't have the fact

    # If nothing is set we process message
    if wf is None and wof is None:
        logging.debug('Match - no filter')
        return False

    # If wof is in facts we return True ( skip message )
    if wof is not None:
        wof = wof.split(',')
        for f in wof:
            if '=' in f:
                f = f.split('=')
    
                if (f[0] in yaml_facts) and (yaml_facts[f[0]] == f[1]):
                    logging.debug('Match - wof match')
                    return True
            else:
                if f in yaml_facts:
                    logging.debug('Match - wof match')
    		    return True

    # Without is set but we did not find it, if wf is not set we return False ( process message )
    if wf is None:
        logging.debug('Match - no wof match')
        return False

    # If all wf is in facts we return False ( process message )
    no_match=0
    wf = wf.split(',')
    for f in wf:
        if '=' in f:
            f = f.split('=')
   
            if (f[0] not in yaml_facts) or (yaml_facts[f[0]] != f[1]):
                no_match += 1
        else:
            if f not in yaml_facts:
                no_match += 1

    if no_match == 0 :
        # All facts was found ( process message )
        logging.debug('Match - wf filter')
        return False
    else:
        # Facts set was not found - return True ( skip message )
        logging.debug('No match - wf filter')
        return True

def fork_worker(task_queue, done_queue ):
    # process control
    P = Process(target=receive_queue_msg, args=(task_queue, done_queue ))
    P.daemon=True
    P.start()
    logging.debug('Forking worker with pid ' + str(P.pid))

    # Calling active_children will also make sure that we clean up old hanging children
    running_children = len(active_children())
    logging.debug(str(running_children) + ' active children')

def check_scheduled_queue(scheduled_queue):
    logging.debug('Checking scheduled queue')

    if scheduled_queue.qsize() > 0:
        try:
            message = scheduled_queue.get(False)
            # Remove scheduled flag
            message['is_scheduled']=False
            when=(int(message['schedule']) - time.time())
            logging.debug('Scheduling execution of task')
            t = Timer(when, process_msg, (message,))
            t.daemon=True
            t.start()
        except Empty:
            logging.debug('scheduled_queue is empty')

def check_task_queue(task_queue, done_queue):
    logging.debug('Checking task queue')
    if task_queue.qsize() > 0:
        fork_worker(task_queue, done_queue )

def check_done_queue(task_queue, done_queue, sqs_write_queue):
    logging.debug('Checking done queue')

    if done_queue.qsize() > 0:
        try:
            response = done_queue.get(False)
            logging.debug('Response read from done queue')
            write_sqs_msg (response, sqs_write_queue)
            task_queue.put('STOP')
        except Empty:
            logging.debug('Done queue is empty')

def schedule_check_done_queue(task_queue, done_queue, sqs_write_queue):
    s = sched.scheduler(time.time, time.sleep)

    try:
        while True:
            s.enter(CFG['general']['sqs_att_poll_interval'], 1, check_done_queue, (task_queue, done_queue, sqs_write_queue))
            s.run()

    except (KeyboardInterrupt, SystemExit):
        logging.info('Done queue checker process exiting...')
        sys.exit(0)

def fork_check_done_queue(task_queue, done_queue, sqs_write_queue):
    # process control
    P = Process(target=schedule_check_done_queue, args=(task_queue, done_queue, sqs_write_queue))
    P.daemon=True
    P.start()
    logging.debug('Forking worker to handle sqs queue with pid ' + str(P.pid))

    # Calling active_children will also make sure that we clean up old hanging children
    running_children = len(active_children())
    logging.debug(str(running_children) + ' active children')

    return P

def sqs_poll_slow_down(poll_control_queue):
    poll_control_queue.put(CFG['general']['sqs_poll_interval'])

def schedule_receive_sqs_msg(sqs_read_queue, task_queue, yaml_queue, scheduled_queue):
    poll_control_queue = Queue()

    s = sched.scheduler(time.time, time.sleep)
    t = []
    sqs_poll_interval = CFG['general']['sqs_poll_interval']
    yaml_facts = None

    # We'll keep old messages here
    global READ_MSGS
    global READ_MSGS_LTS
    READ_MSGS=[]
    READ_MSGS_LTS=time.time()

    try:
        while True:
            # Reload yaml facts if available
            if yaml_queue.qsize() > 0:
                try:
                    yaml_facts = yaml_queue.get(False)
                except Empty:
                    logging.debug('Yaml queue is empty')

            s.enter(sqs_poll_interval, 1, receive_sqs_msg, (sqs_read_queue, task_queue, poll_control_queue, yaml_facts, scheduled_queue))
            s.run()

            # Check if we need to decrease poll sime 
            if poll_control_queue.qsize() > 0:
                try:
                    new_sqs_poll_interval = poll_control_queue.get(False)
                    if ( new_sqs_poll_interval <= sqs_poll_interval ):
                        logging.debug('SQS attention mode ON')
                        sqs_poll_interval = new_sqs_poll_interval
                        for timer in t:
                            logging.debug('Resetting SQS attention timer')
                            timer.cancel()
                        t_init = Timer(CFG['general']['sqs_att_poll_period'], sqs_poll_slow_down, (poll_control_queue,))
                        t_init.daemon=True
                        t_init.start()
                        t.append(t_init)
                    else:
                        logging.debug('SQS attention mode OFF')
                        sqs_poll_interval = new_sqs_poll_interval

                except Empty:
                    logging.debug('poll_control_queue queue is empty')

            # Clear references to old messages ( keep messages of 5 minutes )
            if ( READ_MSGS_LTS < (time.time() - 300 )):
                READ_MSGS_LTS=time.time()
                logging.debug('Emptying list of read messages')
                READ_MSGS=[]

    except (KeyboardInterrupt, SystemExit):
        logging.info('Receive sqs messages process exiting...')
        sys.exit(0)

def fork_receive_sqs_msg(sqs_read_queue, task_queue, yaml_queue, scheduled_queue):
    # process control
    P = Process(target=schedule_receive_sqs_msg, args=(sqs_read_queue, task_queue, yaml_queue, scheduled_queue))
    P.daemon=True
    P.start()
    logging.debug('Forking worker to handle sqs queue with pid ' + str(P.pid))

    # Calling active_children will also make sure that we clean up old hanging children
    running_children = len(active_children())
    logging.debug(str(running_children) + ' active children')

    return P

def main ():
    # Connect with key, secret and region
    conn = connect_to_region(CFG['aws']['region'])
    sqs_read_queue = conn.get_queue(CFG['aws']['sqs_read_queue'])
    sqs_write_queue = conn.get_queue(CFG['aws']['sqs_write_queue'])

    # Create queues
    task_queue = Queue()
    done_queue = Queue()
    scheduled_queue = Queue()
    yaml_queue = Queue()

    # Lets get those facsts ( it's a string not a boolean )
    if CFG['general']['yaml_facts'] == 'True':
        get_yaml_facts(yaml_queue, 'initial')

        # Schedule a refresh
        t_yaml_facts = Timer(CFG['general']['yaml_facts_refresh'], get_yaml_facts, (yaml_queue,))
        t_yaml_facts.daemon=True
        t_yaml_facts.start()

    # Task queue scheduler
    main_task_sched = sched.scheduler(time.time, time.sleep)

    # fork processes for sqs queue check and done queue checking
    proc_receiver=fork_receive_sqs_msg(sqs_read_queue, task_queue, yaml_queue, scheduled_queue)
    proc_done_queue=fork_check_done_queue(task_queue, done_queue, sqs_write_queue)

    while ( True ):
        main_task_sched.enter(0.1, 1, check_task_queue, (task_queue, done_queue ))
        main_task_sched.enter(0.1, 2, check_scheduled_queue, (scheduled_queue, ))
        main_task_sched.run()

        if CFG['general']['yaml_facts'] == 'True' and not t_yaml_facts.is_alive():
            t_yaml_facts = Timer(CFG['general']['yaml_facts_refresh'], get_yaml_facts, (yaml_queue,))
            t_yaml_facts.daemon=True
            t_yaml_facts.start()

        if not proc_receiver.is_alive():
           logging.error ('SQS receiver process just died - restarting')
           proc_receiver=fork_receive_sqs_msg(sqs_read_queue, task_queue, yaml_queue, scheduled_queue)

        if not proc_done_queue.is_alive():
           logging.error ('Done queue checker process just died - restarting')
           proc_done_queue=fork_check_done_queue(task_queue, done_queue, sqs_write_queue)

if __name__ == "__main__":

    (foreground, logfile, pidfile)=initialize()
    daemonize(foreground, pidfile,'/dev/null', logfile, logfile)
    try:
        sys.exit(main())
    except (KeyboardInterrupt, SystemExit):
        logging.info('Main process dutifully exiting...')
        sys.exit(0)
