#!/usr/bin/env python
# coding: utf-8

from socket import gethostname
import simplejson as json
import subprocess
from threading import Timer
import time
import sched
import signal
import sys
import yaml
import os
import stat
import getopt
import logging
import pkg_resources
import fcntl

# S3 stuff
from boto.s3.key import Key
from boto.s3.connection import S3Connection
from boto.exception import S3ResponseError

# SQS stuff
from boto.sqs.connection import SQSConnection
from boto.sqs.regioninfo import SQSRegionInfo
from boto.sqs import connect_to_region

# Forking stuff
from multiprocessing import Process, Queue, active_children
from Queue import Empty

try:
    pkg_resources.require("Boto>=1.9b")
except pkg_resources.VersionConflict, e:
    sys.stderr.write('Boto 1.9b or higher is required - exiting\n')
    sys.exit(1)

CFILE='/etc/ec2_collective/ec2-cagent.json'

# Create queues
task_queue = Queue()
done_queue = Queue()
yaml_queue = Queue()
poll_control_queue = Queue()

def terminate_process(signum, frame):
    logging.info ('Process asked to exit (SIGTERM) - exiting')
    sys.exit(0)

signal.signal(signal.SIGTERM, terminate_process)

# Signal handler
def master_timeout(signum, frame):
    logging.error('AWS request timeout reached (SIGALARM) - exiting')
    sys.exit(1)

# When timeout happens master_timeout definition executes raising an exception
signal.signal(signal.SIGALRM, master_timeout)

def usage():
    sys.stderr.write('Usage:')
    sys.stderr.write('\t' + sys.argv[0] + '\n\n -f, --foreground\trun script in foreground\n -h, --help\tthis help\n -l, --logfile\tlogfile path ( /var/log/ec2_collective/ec2-cagent.log )\n -p, --pidfile\tpath to pidfile (/var/run/ec2-cagent.pid)\n')
    sys.exit(1)

def set_logging(logfile, foreground):

    log = logging.getLogger()
    logging.getLogger('boto').setLevel(logging.CRITICAL)
    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")

    if foreground is True:
        ch  = logging.StreamHandler(sys.stdout)
        ch.setFormatter(fmt)
        log.addHandler(ch)
    else:
        fh = logging.handlers.WatchedFileHandler(logfile)
        fh.setFormatter(fmt)
        log.addHandler(fh)

    if CFG['general']['log_level'] not in ['INFO', 'WARN', 'ERROR', 'DEBUG', 'CRITICAL' ]:
        sys.stderr.write('Log level: ' + CFG['general']['log_level'] + ' is invalid\n')
        sys.exit(1)

    if CFG['general']['log_level'] == 'INFO':
        logging.getLogger().setLevel(logging.INFO)
    elif CFG['general']['log_level'] == 'WARN':
        logging.getLogger().setLevel(logging.WARNING)
    elif CFG['general']['log_level'] == 'ERROR':
        logging.getLogger().setLevel(logging.ERROR)
    elif CFG['general']['log_level'] == 'DEBUG':
        logging.getLogger().setLevel(logging.DEBUG)
    elif CFG['general']['log_level'] == 'CRITICAL':
        logging.getLogger().setLevel(logging.CRITICAL)

def acquire_lock(pidfile):

    try:
        f = open(pidfile, 'a')
    except IOError, err:
        sys.stderr.write('Unable to open lockfile: ' + str(pidfile) + ' (' + str(err) + ')\n')
        sys.exit(1)

    try:
        fcntl.flock(f, fcntl.LOCK_EX|fcntl.LOCK_NB)
        return f
    except IOError, err:
        sys.stderr.write('Unable to acquire lockfile: ' + str(pidfile) + ' (' + str(err) + ')\n')
        sys.exit(1)

def initialize(logfile='/var/log/ec2_collective/ec2-cagent.log', pidfile='/var/run/ec2-cagent.pid', foreground=False):

    try:
        opts, args = getopt.getopt(sys.argv[1:], 'hfl:p:', ['foreground', 'help', 'logfile', 'pidfile'])

    except getopt.GetoptError, err:
        sys.stderr.write('Failed to parse arguments: (%s)\n' % (err))
        sys.exit(1)

    for o, a in opts:
        if o in ('-h', '--help'):
            usage()
        elif o in ('-f', '--foreground'):
            foreground=True
        elif o in ('-l', '--logfile'):
            logfile=a
        elif o in ('-p', '--pidfile'):
            pidfile=a

    get_config()
    set_logging(logfile,foreground)

    stdoutlog = os.path.dirname(logfile) + '/ec2-cagent.stdout'
    stderrlog = os.path.dirname(logfile) + '/ec2-cagent.stderr'

    return (foreground, logfile, pidfile, stdoutlog, stderrlog)

def write_yaml_to_queue ( message, sqs_facts_queue ):

    # Make sure the facts message contains a hostname
    message.update({ 'ec2_cagent_hostname' : gethostname() })

    logging.debug('write_yaml_to_queue: Writing facts to sqs facts queue')
    write_sqs_msg (message, sqs_facts_queue)

def get_yaml_facts (sqs_facts_queue):
    logging.debug('get_yaml_facts: Loading facts')
 
    # Define dict for storage
    dataMap = {}
    valid_yaml_files = []

    yaml_files = CFG['general']['yaml_facts_path'].split(',')
    for yaml_file in yaml_files:

        if not os.path.exists(yaml_file):
            logging.error( yaml_file + ' file does not exist')
            continue

        # If we get to here - the file looks good
        valid_yaml_files.append(yaml_file)

    for yaml_file in valid_yaml_files:
        logging.debug('get_yaml_facts: Will load facts from ' + str(yaml_file))

        f = open(yaml_file, 'r')
        try:
            data_from_file = yaml.safe_load(f)
        except:
            logging.error('Error while loading yaml from file (format problems) ' + str(yaml_file))
            continue

        if type(data_from_file) is str:
            logging.debug('get_yaml_facts: Facts file looks to be string ' + str(yaml_file))
            fact_string = data_from_file.split()
        
            for fact in fact_string:
                newfact = {fact:'None'}
                dataMap.update(newfact)
        else:
            logging.debug('get_yaml_facts: We assume facts file is in yaml format ' + str(yaml_file))
            dataMap.update(data_from_file)
            f.close()

    if len(dataMap) > 0:
        logging.info('Facts loaded from file(s)')
        logging.debug('get_yaml_facts:' + str(dataMap))
        yaml_queue.put(dataMap)

        if CFG['general']['use_facts_queue'] == True:
            write_yaml_to_queue(dataMap, sqs_facts_queue)

        return True
    else:
        logging.debug('get_yaml_facts: Facts are empty')
        return False

def cli_mode (message):

    try:
        logging.debug ('cli_mode: Executing: ' + str(message['cmd']))
        o = subprocess.Popen(message['cmd'], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        output = o.communicate()[0]
        rc = o.poll()

    except OSError, e:
        output = ('Failed to execute ' + message['cmd'] + ' (%d) %s \n' % (e.errno, e.strerror))
        rc = e.errno 

    response={'mode': message['mode'], 'output': output, 'rc': rc, 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
    return response

def write_to_file (payload, identifier):
    script_file = '/tmp/' + 'ec2-cagent-' + identifier

    try:
        f = open (script_file, 'w')
        f.write (payload)
        f.close()
    except IOError, e:
        logging.error ('Failed to write payload to ' + script_file + ' (%d) %s \n' % (e.errno, e.strerror))
        return (False, 'Failed to write payload to file ' + script_file)

    os.chmod(script_file, stat.S_IRWXU)

    return (True, script_file)

def fetch_s3_file ( s3_path, identifier ):

    bucket_name=os.path.dirname(s3_path)
    file_name=os.path.basename(s3_path)

    logging.debug ('fetch_s3_file: Fetching script ' + file_name + ' from s3 bucket ' + bucket_name )

    try:
        s3conn =S3Connection()
    except S3ResponseError:
        logging.error ('Failed to connect to S3')
        return (False, 'Failed to connect to S3')
    
    try:
        bucket = s3conn.get_bucket(bucket_name)
    except S3ResponseError:
        logging.error ('Failed to get bucket ' + bucket_name )
        return (False, 'Failed to get bucket ' + bucket_name )

    k = Key(bucket)
    k.key = file_name

    try:
        payload = k.get_contents_as_string()
    except S3ResponseError, e:
        logging.error ('Failed to get file from bucket ' + str(e))
        return (False, 'Failed to get file from bucket')

    return write_to_file(payload, identifier )

def script_mode (message):

    # Put script on filesystem
    if message['mode'] == 's3':
        (file_written, script_file ) = fetch_s3_file(message['cmd'], message['batch_msg_id'])
    else:
        (file_written, script_file ) = write_to_file(message['payload'], message['batch_msg_id'])

    if file_written is False:
        # script_file will include error message
        response={'mode': message['mode'], 'output': script_file, 'rc': '255', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
        return response
  
    command=script_file 

    # If we are carrying script parameters, please use them
    if message['script_param'] is not None:
        command = str(command) + ' ' + str(message['script_param'])

    try:
        logging.debug ('script_mode: Executing script: ' + str(message['cmd']) + ' with params: ' + str(message['script_param']))
        o = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        output = o.communicate()[0]
        rc = o.poll()

    except OSError, e:
        logging.error ('Failed to execute ' + message['cmd'] )
        output = ('Failed to execute ' + message['cmd'] + ' (%d) %s \n' % (e.errno, e.strerror))
        rc = e.errno 

    try:
        logging.debug ('script_mode: Deleting file with script '  + script_file)
        os.remove(script_file)
    except OSError, e:
        logging.error ('Failed to delete script file ' + script_file + ' (%d) %s \n' % (e.errno, e.strerror))

    response={'mode': message['mode'], 'output': output, 'rc': rc, 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
    return response

def receive_sqs_msg ( sqs_read_queue, yaml_facts ):
    global READ_MSGS
    global READ_MSGS_LTS

    logging.debug('receive_sqs_msg: Looking for new messages on SQS')

    # This call might never return...
    signal.alarm(CFG['general']['timeout'])
    logging.info ('Attempting to get messages from SQS')
    msgs = sqs_read_queue.get_messages(num_messages=10, visibility_timeout=0)
    signal.alarm(0)

    for msg in msgs:
        READ_MSGS_LTS=time.time()

        if msg.id in READ_MSGS:
            continue
        else:
            READ_MSGS.append(msg.id)
   
        message=json.loads(msg.get_body())
        batch_msg_id = message['batch_msg_id']
        wf = message['wf']
        wof = message['wof']
        message['orgid'] = msg.id

        # We send multiple duplicate messages - lets avoid handling all of them
        if batch_msg_id in READ_MSGS:
            continue
        else:
            READ_MSGS.append(batch_msg_id)
  
        if fact_lookup(wf, wof, yaml_facts ):
            logging.debug('receive_sqs_msg: SQS messages did not pass filter')
            READ_MSGS.append(msg.id)
            continue

        logging.debug('receive_sqs_msg: New valid SQS message received')

        logging.debug('receive_sqs_msg: Putting SQS message on task queue')
        task_queue.put(message)

        poll_control_queue.put(CFG['general']['sqs_att_poll_interval'])


def process_msg ( message ):

    cmd_str = str(message['cmd'])
    mode = str(message['mode'])
    ts = str(message['ts'])

    if mode in ['ping', 'count']:
        logging.info('Performing ' + str(mode) + ' reply')
        response={'mode': mode, 'output': ts, 'rc': '0', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};
        return response
    else:
        logging.info("Performing ping reply - for discovery purpose")
        response={'mode': 'ping', 'output': ts, 'rc': '0', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname(), 'discovery':1};
        done_queue.put(response)

    if mode == 'cli':
        logging.info("Performing command execution")
        response = cli_mode(message)
    elif mode in [ 'script', 's3' ]:
        logging.info('Performing ' + mode + ' execution')
        response = script_mode (message) 
    else:
        logging.error('Unknown mode: ' + mode)
        response =  'Unknown command ' + cmd_str
        response={'mode': mode, 'output': response, 'rc': '0', 'ts':time.time(), 'msg_id':message['orgid'], 'hostname':gethostname()};

    return response

def receive_queue_msg ( ):

    for message in iter(task_queue.get, 'STOP'):
        logging.debug('receive_queue_msg: Task read from task queue')
        done_queue.put(process_msg(message))

    logging.debug('receive_queue_msg: Worker received STOP signal - terminating')

def write_sqs_msg (response, sqs_write_queue):

    # response - dict
    # response_in_json - json string
    # message - sqs message object

    try:
        response_in_json=json.dumps(response)
        logging.debug ('write_sqs_msg: Response size in json ' + str(len(response_in_json)))
        message = sqs_write_queue.new_message(response_in_json)
    except:
        logging.error('Failed to create message object - check queue name')
        return False

    if len(message) >= 65536 and 'output' in response:
        logging.warn ('Response message is too big to put on SQS ' + str(len(message)) + ' - sending last 1000 characters')

        # Let's make sure we can actually decrease response....
        if len(response['output']) > 1000:
            response['output'] = '--- OUTPUT CAPPED ---\n' + str(response['output'][-1000:])
        else:
            logging.error ('Message size is too big to put on sqs, but it is not in the actual output!')
            response['output'] = 'Panic response from agent - please check exit code'

        try:
            response_in_json=json.dumps(response)
            logging.debug ('write_sqs_msg: Response size in json ' + str(len(response_in_json)))
            message = sqs_write_queue.new_message(response_in_json)
        except:
            logging.error('Failed to create message object - check queue name')
            return False

    elif len(message) >= 65536:
        logging.error ('Response message is too big to put on SQS ' + str(len(message)))
        return False
    
    # Retry 3 times
    for i in range(0, 3):
        signal.alarm(CFG['general']['timeout'])
        logging.info ('Attempting write to SQS')
        org = sqs_write_queue.write(message)
        signal.alarm(0)
        if org.id is None:
            logging.error ('Failed to write response message to SQS')
            time.sleep(1)
        else:
            logging.debug('write_sqs_msg: Wrote response to SQS')
            return True

    return False

def get_config():
    # CFILE 
    if not os.path.exists(CFILE):
        sys.stderr.write('Config file (%s) does not exist\n' % (CFILE) )
        sys.exit(1)
    
    try:
        f = open(CFILE, 'r')
    except IOError, e:
        sys.stderr.write('Failed to open config file (%s)\n' % (CFILE) )
        sys.exit(1)
    
    try:
        global CFG
        CFG=json.load(f)
    except (TypeError, ValueError), e:
        sys.stderr.write('Error in configuration file (%s)\n' % (CFILE) )
        sys.exit(1)

def write_pidfile(pidfile):

    try:
        f = open (pidfile, 'w')
        f.write (str(os.getpid()))
    except IOError, e:
        sys.stderr.write("Failed to write pid to pidfile (%s): (%d) %s\n" % (pidfile, e.errno, e.strerror))
        sys.exit(1)

def daemonize (foreground, pidfile, stdin='/dev/null', stdout='/dev/null', stderr='/dev/null' ):
    lockfile=acquire_lock(pidfile)

    if foreground is True:
        logging.info ('Running in the foreground')
        write_pidfile(pidfile)
        return lockfile

    try:
        pid = os.fork( )
        if pid > 0:
            sys.exit(0) # Exit first parent.
    except OSError, e:
        sys.stderr.write("fork #1 failed: (%d) %s\n" % (e.errno, e.strerror))
        sys.exit(1)
    # Decouple from parent environment.
    os.chdir("/")
    os.umask(0)
    os.setsid( )
    # Perform second fork.
    try:
        pid = os.fork( )
        if pid > 0:
            sys.exit(0) # Exit second parent.
    except OSError, e:
        sys.stderr.write("fork #2 failed: (%d) %s\n" % (e.errno, e.strerror))
        sys.exit(1)
    # The process is now daemonized, redirect standard file descriptors.
    for f in sys.stdout, sys.stderr: f.flush( )
    si = file(stdin, 'r')
    so = file(stdout, 'a+')
    se = file(stderr, 'a+', 0)
    os.dup2(si.fileno( ), sys.stdin.fileno( ))
    os.dup2(so.fileno( ), sys.stdout.fileno( ))
    os.dup2(se.fileno( ), sys.stderr.fileno( ))

    write_pidfile(pidfile)

    sys.stdout.write('Daemon started with pid %d\n' % os.getpid( ) )

    sys.stderr.flush()
    sys.stdout.flush()

    return lockfile

def fact_lookup (wf, wof, yaml_facts ):

    # True - Skip
    # False - Process

    # WOF
    # Return True if we have the fact ( just on should skip message )
    # Return False if we don't have the fact

    # WF
    # Return False if we have all the fact ( all facts must match )
    # Return True if we don't have the fact

   # Execptions where we need to be careful

    # If yaml facts is false 
    if CFG['general']['yaml_facts'] == False:
        if wf is None and wof is None:
            logging.debug('fact_lookup: Process message - yaml_facts is False and message contains no facts')
            return False
        else:
            logging.debug('fact_lookup: Skip message - yaml_facts is False but message contains facts')
            return True

    # If yaml facts is true 
    if CFG['general']['yaml_facts'] == True: 
        # but we have no facts skip everything
        if yaml_facts is None:
            logging.error('Skip message - yaml_facts is True but there are no facts available!')
            return True

        # If nothing is set we process message
        if wf is None and wof is None:
            logging.debug('fact_lookup: Process message - yaml_facts is True but message contains no facts')
            return False

    # If wof is in facts we return True ( skip message )
    if wof is not None:
        wof = wof.split(',')
        for f in wof:
            if '=' in f:
                f = f.split('=')
    
                if (f[0] in yaml_facts) and (yaml_facts[f[0]] == f[1]):
                    logging.debug('fact_lookup: Skip message - wof matched')
                    return True
            else:
                if f in yaml_facts:
                    logging.debug('fact_lookup: Skip message - wof matched')
                    return True

    # Without is set but we did not find it, if wf is not set we return False ( process message )
    if wf is None:
        logging.debug('fact_lookup: Process message - no wof match and no wf')
        return False

    # If all wf is in facts we return False ( process message )
    no_match=0
    wf = wf.split(',')
    for f in wf:
        if '=' in f:
            f = f.split('=')
   
            if (f[0] not in yaml_facts) or (yaml_facts[f[0]] != f[1]):
                no_match += 1
        else:
            if f not in yaml_facts:
                no_match += 1

    if no_match == 0 :
        # All facts was found ( process message )
        logging.debug('fact_lookup: Process message - wf matched')
        return False
    else:
        # Facts set was not found - return True ( skip message )
        logging.debug('fact_lookup: Skip message - no wf match')
        return True

def fork_worker():
    # process control
    P = Process(target=receive_queue_msg, args=())
    P.daemon=True
    P.start()
    logging.debug('fork_worker: Forking worker with pid ' + str(P.pid))

def check_task_queue():
    logging.debug('check_task_queue: Checking task queue')
    if task_queue.qsize() > 0:
        fork_worker()

def check_done_queue(sqs_write_queue):
    logging.debug('check_done_queue: Checking done queue')

    if done_queue.qsize() > 0:
        try:
            response = done_queue.get(False)
            logging.debug('check_done_queue: Response read from done queue')
            write_sqs_msg (response, sqs_write_queue)

            if 'discovery' in response and response['discovery'] == 1:
                logging.debug('check_done_queue: Discovery message sent - not stopping worker')
            else:
                task_queue.put('STOP')
        except Empty:
            logging.debug('check_done_queue: Done queue is empty')

def schedule_check_done_queue():

    sqs_write_queue = establish_sqs_conn(CFG['aws']['sqs_write_queue'])

    s = sched.scheduler(time.time, time.sleep)

    try:
        while True:
            s.enter(CFG['general']['sqs_att_poll_interval'], 1, check_done_queue, (sqs_write_queue,))
            s.run()

    except (KeyboardInterrupt, SystemExit):
        logging.info('Done queue checker process exiting...')
        sys.exit(0)

def schedule_get_yaml_facts():

    sqs_facts_queue = establish_sqs_conn(CFG['aws']['sqs_facts_queue'])

    # Get the facts initially as quickly as possible
    get_yaml_facts(sqs_facts_queue)

    s = sched.scheduler(time.time, time.sleep)

    try:
        while True:
            s.enter(CFG['general']['yaml_facts_refresh'], 1, get_yaml_facts, (sqs_facts_queue,))
            s.run()

    except (KeyboardInterrupt, SystemExit):
        logging.info('Get facts process exiting...')
        sys.exit(0)

def fork_check_done_queue():
    # process control
    P = Process(target=schedule_check_done_queue, args=())
    P.daemon=True
    P.start()
    logging.debug('fork_check_done_queue: Forking worker to handle done queue with pid ' + str(P.pid))

    # Calling active_children will also make sure that we clean up old hanging children
    running_children = len(active_children())
    logging.debug('fork_check_done_queue: ' + str(running_children) + ' active children')

    return P

def sqs_poll_slow_down():
    poll_control_queue.put(CFG['general']['sqs_poll_interval'])

def schedule_receive_sqs_msg():

    # Make connectoin for this child
    sqs_read_queue = establish_sqs_conn(CFG['aws']['sqs_read_queue'])

    s = sched.scheduler(time.time, time.sleep)
    t = []
    sqs_poll_interval = CFG['general']['sqs_poll_interval']
    yaml_facts = None

    # We'll keep old messages here
    global READ_MSGS
    global READ_MSGS_LTS
    READ_MSGS=[]
    READ_MSGS_LTS=time.time()

    try:
        while True:
            # Reload yaml facts if available
            if yaml_queue.qsize() > 0:
                try:
                    yaml_facts = yaml_queue.get(False)
                except Empty:
                    logging.debug('schedule_receive_sqs_msg: Yaml queue is empty')

            s.enter(sqs_poll_interval, 1, receive_sqs_msg, (sqs_read_queue, yaml_facts))
            s.run()

            # Check if we need to decrease poll sime 
            if poll_control_queue.qsize() > 0:
                try:
                    new_sqs_poll_interval = poll_control_queue.get(False)
                    if ( new_sqs_poll_interval <= sqs_poll_interval ):
                        logging.debug('schedule_receive_sqs_msg: SQS attention mode ON')
                        sqs_poll_interval = new_sqs_poll_interval
                        for timer in t:
                            logging.debug('schedule_receive_sqs_msg: Resetting SQS attention timer')
                            timer.cancel()
                            timer.join(1.0)
                            t.remove(timer)
                        t_init = Timer(CFG['general']['sqs_att_poll_period'], sqs_poll_slow_down, ())
                        t_init.daemon=True
                        t_init.start()
                        t.append(t_init)
                    else:
                        logging.debug('schedule_receive_sqs_msg: SQS attention mode OFF')
                        sqs_poll_interval = new_sqs_poll_interval

                except Empty:
                    logging.debug('schedule_receive_sqs_msg: poll_control_queue queue is empty')

            # Clear references to old messages ( keep messages until we haven't done anything for 6 minutes ) 
            # AWS SQS queues must have this setting
            # Message Retention Period:     5 minutes
            # That will allow AWS to purge old messages a minute before we might pick it up again.
            if ( READ_MSGS_LTS < (time.time() - 360 )):
                READ_MSGS_LTS=time.time()
                logging.debug('schedule_receive_sqs_msg: Emptying list of read messages')
                READ_MSGS=[]

    except (KeyboardInterrupt, SystemExit):
        logging.info('Receive sqs messages process exiting...')
        sys.exit(0)

def fork_receive_sqs_msg():
    # process control
    P = Process(target=schedule_receive_sqs_msg, args=())
    P.daemon=True
    P.start()
    logging.debug('fork_receive_sqs_msg: Forking worker to handle sqs queue with pid ' + str(P.pid))

    # Calling active_children will also make sure that we clean up old hanging children
    running_children = len(active_children())
    logging.debug('fork_receive_sqs_msg: ' + str(running_children) + ' active children')

    return P

def fork_get_yaml_facts():
    # process control
    P = Process(target=schedule_get_yaml_facts, args=())
    P.daemon=True
    P.start()
    logging.debug('fork_get_yaml_facts: Forking worker to periodically refresh facts with pid ' + str(P.pid))

    # Calling active_children will also make sure that we clean up old hanging children
    running_children = len(active_children())
    logging.debug('fork_get_yaml_facts: ' + str(running_children) + ' active children')

    return P

def establish_sqs_conn (queue):

    # Connect with key, secret and region
    try:
        logging.info('Connecting to region ' + str(CFG['aws']['region']))
        signal.alarm(CFG['general']['timeout'])
        conn = connect_to_region(CFG['aws']['region'])
        signal.alarm(0)
    except:
        logging.error('Could not connect to SQS - check your authentication')
        sys.exit(1)

    try:
        logging.info('Getting queue url for ' + str(queue))
        signal.alarm(CFG['general']['timeout'])
        queue = conn.get_queue(queue)
        signal.alarm(0)
    except:
        logging.error('Could not determine SQS queue url - check your authentication')
        sys.exit(1)

    if queue is None:
        logging.error('Unable to get ' + str(queue) + ' queue by name')
        sys.exit(1)

    return queue

def main ():

    # Task queue scheduler
    main_task_sched = sched.scheduler(time.time, time.sleep)

    # fork processes for sqs queue check, done queue checking and yaml facts
    proc_receiver=fork_receive_sqs_msg()
    proc_done_queue=fork_check_done_queue()

    if CFG['general']['yaml_facts'] == True:
        proc_get_yaml_facts=fork_get_yaml_facts()

    while ( True ):
        main_task_sched.enter(0.1, 1, check_task_queue, ())
        main_task_sched.run()

        # Calling active_children will also make sure that we clean up old hanging children
        running_children = len(active_children())
        logging.debug('main: ' + str(running_children) + ' active children')

        if CFG['general']['yaml_facts'] == True and not proc_get_yaml_facts.is_alive():
            logging.error ('Get yaml facts process just died - restarting')
            proc_get_yaml_facts=fork_get_yaml_facts()

        if not proc_receiver.is_alive():
            logging.error ('SQS receiver process just died - restarting')
            proc_receiver=fork_receive_sqs_msg()

        if not proc_done_queue.is_alive():
            logging.error ('Done queue checker process just died - restarting')
            proc_done_queue=fork_check_done_queue()

if __name__ == "__main__":

    (foreground, logfile, pidfile, stdoutlog, stderrlog)=initialize()
    lockfile=daemonize(foreground, pidfile,'/dev/null', stdoutlog, stderrlog)
    try:
        sys.exit(main())
    except (KeyboardInterrupt, SystemExit):
        logging.info('Main process dutifully exiting...')
        sys.exit(0)
